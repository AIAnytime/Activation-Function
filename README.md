# Activation-Function
An implementation of different activation functions like Sigmoid, tanh, ReLU, Linear, and Softmax. This repository also describes which activation function to use in hidden layer and the output layers based on the neural network architectures.
